{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original dataset: Opinosis Opinion ⁄ Review Data Set https://archive.ics.uci.edu/ml/datasets/Opinosis+Opinion+%26frasl%3B+Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import itertools\n",
    "import hashlib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 text files we choose to use\n",
    "file_list = ['battery-life_amazon_kindle',\n",
    "            'battery-life_ipod_nano_8gb',\n",
    "            'location_bestwestern_hotel_sfo',\n",
    "            'location_holiday_inn_london',\n",
    "            'price_amazon_kindle',\n",
    "            'room_holiday_inn_london',\n",
    "            'rooms_bestwestern_hotel_sfo',\n",
    "            'screen_ipod_nano_8gb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O function\n",
    "def import_dataset(file_name):\n",
    "    \"\"\"\n",
    "    input: file_name in file_list\n",
    "    \"\"\"\n",
    "    \n",
    "    dataFile = './dataset/' + str(file_name) + '.txt.data'\n",
    "    with open(dataFile, 'rb') as f:\n",
    "        contents = []\n",
    "        for line in f.readlines():\n",
    "            contents.append(line.strip().decode('utf-8'))\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataset\n",
      "document number:8\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "print('Importing dataset')\n",
    "\n",
    "dataset = [] # 8-entry list, each corresponding to a file\n",
    "for file_name in file_list:\n",
    "    contents = import_dataset(file_name)\n",
    "    dataset.append(contents)\n",
    "    \n",
    "print('document number:' + str(len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A class Shingling that constructs k–shingles of a given length k (e.g., 10) from a given document, computes a hash value for each unique shingle, and represents the document in the form of an ordered set of its hashed k-shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function constructs k-shingles from a given text\n",
    "def shingling(text, k=5):\n",
    "    \"\"\"\n",
    "    input: text in dataset\n",
    "            k, shingle size.\n",
    "    \"\"\"\n",
    "    \n",
    "    # split the string into separate words\n",
    "    split_text = []\n",
    "    for review in text:\n",
    "        exclude = set(string.punctuation)\n",
    "        review = ''.join(ch for ch in review if ch not in exclude) # remove punctuation\n",
    "        review = review.lower() # convert all characters into lower characters\n",
    "        #for word in review.split():\n",
    "        #    split_text.append(word)\n",
    "        for character in list(review):\n",
    "            split_text.append(character)\n",
    "    \n",
    "    # k-shingle\n",
    "    shingle_list = []\n",
    "    for i in range(len(split_text)-k+1):\n",
    "        shingle_list.append(split_text[i:i+k])\n",
    "\n",
    "    # remove duplicates\n",
    "    shingle_list.sort()\n",
    "    shingle_no_dup = list(shingle_list for shingle_list,_ in itertools.groupby(shingle_list))\n",
    "    \n",
    "    # each sublist in shingle_no_dup represents a shingle\n",
    "    # convert the sublist into a string\n",
    "    shingle_strings = []\n",
    "    for index, shingle in enumerate(shingle_no_dup):\n",
    "        sum_string = shingle[0]\n",
    "        for i in range(1, len(shingle)):\n",
    "            sum_string = sum_string + ' ' + shingle[i]\n",
    "        shingle_strings.append(sum_string)\n",
    "        \n",
    "    return shingle_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingling\n",
      "number of shingles in document 1 is 4838\n",
      "number of shingles in document 2 is 3228\n",
      "number of shingles in document 3 is 8507\n",
      "number of shingles in document 4 is 9910\n",
      "number of shingles in document 5 is 5360\n",
      "number of shingles in document 6 is 17206\n",
      "number of shingles in document 7 is 9323\n",
      "number of shingles in document 8 is 3247\n"
     ]
    }
   ],
   "source": [
    "# shingling all 8 texts\n",
    "print('Shingling')\n",
    "\n",
    "shingle_texts = []\n",
    "for text in dataset:\n",
    "    shingle_texts.append(shingling(text))\n",
    "    \n",
    "for i in range(len(shingle_texts)):\n",
    "    num = len(shingle_texts[i])\n",
    "    print(f'number of shingles in document {i+1} is {num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of shingles: 61619\n",
      "number of unique shingles: 30623\n"
     ]
    }
   ],
   "source": [
    "# flatting the list of all shingles\n",
    "flat_shingle_texts = np.hstack(np.array(shingle_texts))\n",
    "print('number of shingles:',flat_shingle_texts.shape[0])\n",
    "\n",
    "# get unique shingles\n",
    "unique_flat_shingle_texts = np.unique(flat_shingle_texts)\n",
    "max_value = unique_flat_shingle_texts.shape[0]\n",
    "print('number of unique shingles:', max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash unique shingles\n",
    "# build dictionary with {unique shingle: hash value}\n",
    "shingle_dict = {}\n",
    "for i in range(len(unique_flat_shingle_texts)):\n",
    "    shingle_dict[unique_flat_shingle_texts[i]] = hash(unique_flat_shingle_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represents the document in the form of an ordered set of its hashed k-shingles\n",
    "shingle_int = shingle_texts\n",
    "for i in range(len(shingle_int)):\n",
    "    for j in range(len(shingle_int[i])):\n",
    "        shingle_int[i][j] = shingle_dict[shingle_texts[i][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A class CompareSets that computes the Jaccard similarity of two sets of integers – two sets of hashed shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Jaccard similarity of two sets of hashed shingles\n",
    "def jaccard_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    input: text1, text2 are entries in shingle_int. eg: text1 = shingle_int[0]\n",
    "    \"\"\"\n",
    "    set1 = set(text1)\n",
    "    set2 = set(text2)\n",
    "    jaccard = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "    return jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard similarity for document 'battery-life_amazon_kindle' and 'battery-life_ipod_nano_8gb' is 0.17323636363636363.\n"
     ]
    }
   ],
   "source": [
    "# test example\n",
    "i = 0 # doc1 index\n",
    "j = 1 # doc2 index\n",
    "jaccard = jaccard_similarity(shingle_int[i],shingle_int[j])\n",
    "\n",
    "namei = file_list[i] # doc1 name\n",
    "namej = file_list[j] # doc2 name\n",
    "print(f\"Jaccard similarity for document '{namei}' and '{namej}' is {jaccard}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Jaccard similarity matrix\n",
    "jaccard_similarity_matrix = np.zeros((len(file_list), len(file_list)))\n",
    "for i in range(len(file_list)):\n",
    "    for j in range(len(file_list)):\n",
    "        jaccard_similarity_matrix[i][j] = jaccard_similarity(\n",
    "            shingle_int[i],shingle_int[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.17323636, 0.14667469, 0.15643378, 0.19948247,\n",
       "        0.14878316, 0.15092653, 0.14146548],\n",
       "       [0.17323636, 1.        , 0.12253683, 0.12492508, 0.14858901,\n",
       "        0.10843504, 0.12182696, 0.22493379],\n",
       "       [0.14667469, 0.12253683, 1.        , 0.29051923, 0.16627418,\n",
       "        0.23146552, 0.29381032, 0.11412322],\n",
       "       [0.15643378, 0.12492508, 0.29051923, 1.        , 0.1744347 ,\n",
       "        0.28572783, 0.26566202, 0.12165388],\n",
       "       [0.19948247, 0.14858901, 0.16627418, 0.1744347 , 1.        ,\n",
       "        0.16313592, 0.17238901, 0.14378738],\n",
       "       [0.14878316, 0.10843504, 0.23146552, 0.28572783, 0.16313592,\n",
       "        1.        , 0.28581815, 0.09991933],\n",
       "       [0.15092653, 0.12182696, 0.29381032, 0.26566202, 0.17238901,\n",
       "        0.28581815, 1.        , 0.11515259],\n",
       "       [0.14146548, 0.22493379, 0.11412322, 0.12165388, 0.14378738,\n",
       "        0.09991933, 0.11515259, 1.        ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A class MinHashing that builds a minHash signature (in the form of a vector or a set) of a given length n from a given set of integers (a set of hashed shingles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test prime\n",
    "def isPrime(c):\n",
    "    \"\"\"\n",
    "    input: n, an arbitrary number to test prime.\n",
    "    \"\"\"\n",
    "    if c==2 or c==3: return True\n",
    "    if c%2==0 or c<2: return False\n",
    "    for i in range(3, int(c**0.5)+1, 2):   # only odd numbers\n",
    "        if c%i==0:\n",
    "            return False    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate n random hash functions in the form ax+b mod c\n",
    "# where c is a prime number\n",
    "def random_hash(n, max_value=100000):\n",
    "    \"\"\"\n",
    "    input: argument n, number of hash functions.\n",
    "            max_value, maximum value can be picked randomly.\n",
    "    \"\"\"\n",
    "    \n",
    "    rand_list = np.zeros((n,3)) # nX3 matrix, columns corresponding to a, b and c\n",
    "    rand_list = rand_list.tolist()\n",
    "    \n",
    "    for i in range(n):\n",
    "        a = random.randint(1, max_value) \n",
    "        b = random.randint(1, max_value) \n",
    "        primes = [i for i in range(1,max_value) if isPrime(i)]\n",
    "        c = random.choice(primes) \n",
    "        \n",
    "        while [a,b,c] in rand_list:\n",
    "            a = random.randint(1, max_value) \n",
    "            b = random.randint(1, max_value) \n",
    "            primes = [i for i in range(1,max_value) if isPrime(i)]\n",
    "            c = random.choice(primes) \n",
    "        \n",
    "        rand_list[i] = [a, b, c]\n",
    "    \n",
    "    return rand_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of signature matrix: (1000, 8)\n"
     ]
    }
   ],
   "source": [
    "# minhashing: build signature matrix\n",
    "# SIG(i,c) is element for the ith hash function and column c\n",
    "num_hash = 1000\n",
    "rand_list = random_hash(num_hash) # each entry is a [a, b, c] list\n",
    "\n",
    "signature_matrix = np.zeros((num_hash, len(file_list)))\n",
    "for j, text in enumerate(shingle_int): # for each column/text\n",
    "    #print(j)\n",
    "    \n",
    "    for i in range(num_hash): # for each row/hash function\n",
    "        minhash = np.inf # initially set SIG(i,c) to inf for all\n",
    "        for element in text: # iteratively compare and keep the smaller of SIG(i,c) and hi(r)\n",
    "            new_hash = (rand_list[i][0] * element + rand_list[i][1]) % rand_list[i][2] # ax+b mod c\n",
    "            #print(rand_list[i], new_hash)\n",
    "            \n",
    "            if new_hash < minhash:\n",
    "                minhash = new_hash\n",
    "                #print(minhash)\n",
    "                \n",
    "        signature_matrix[i][j] = minhash\n",
    "print(f'shape of signature matrix: {signature_matrix.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. A class CompareSignatures that estimates similarity of two integer vectors – minhash signatures – as a fraction of components, in which they agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare columns in signature matrix\n",
    "def vector_similarity(vector1, vector2):\n",
    "    count = 0\n",
    "    for i in range(len(vector1)):\n",
    "        if vector1[i] == vector2[i]:\n",
    "            count += 1\n",
    "    return count/len(vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build signature similarity matrix\n",
    "signature_similarity_matrix = np.zeros((len(file_list), len(file_list)))\n",
    "trans_signature_matrix = np.transpose(signature_matrix)\n",
    "for i in range(len(file_list)):\n",
    "    for j in range(len(file_list)):\n",
    "        signature_similarity_matrix[i][j] = vector_similarity(\n",
    "            trans_signature_matrix[i], trans_signature_matrix[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.    0.252 0.275 0.304 0.307 0.28  0.277 0.234]\n",
      " [0.252 1.    0.229 0.229 0.243 0.209 0.218 0.277]\n",
      " [0.275 0.229 1.    0.444 0.302 0.39  0.437 0.237]\n",
      " [0.304 0.229 0.444 1.    0.297 0.449 0.412 0.236]\n",
      " [0.307 0.243 0.302 0.297 1.    0.288 0.29  0.25 ]\n",
      " [0.28  0.209 0.39  0.449 0.288 1.    0.418 0.239]\n",
      " [0.277 0.218 0.437 0.412 0.29  0.418 1.    0.218]\n",
      " [0.234 0.277 0.237 0.236 0.25  0.239 0.218 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(signature_similarity_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (Optional task for extra 2 bonus) A class LSH that implements the LSH technique: given a collection of minhash signatures (integer vectors) and a similarity threshold t, the LSH class (using banding and hashing) finds all candidate pairs of signatures that agree on at least fraction t of their components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(1,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
