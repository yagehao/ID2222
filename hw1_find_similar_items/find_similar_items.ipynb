{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "original dataset: Opinosis Opinion ⁄ Review Data Set https://archive.ics.uci.edu/ml/datasets/Opinosis+Opinion+%26frasl%3B+Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import itertools\n",
    "import hashlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8 text files we choose to use\n",
    "file_list = ['battery-life_amazon_kindle',\n",
    "            'battery-life_ipod_nano_8gb',\n",
    "            'location_bestwestern_hotel_sfo',\n",
    "            'location_holiday_inn_london',\n",
    "            'price_amazon_kindle',\n",
    "            'room_holiday_inn_london',\n",
    "            'rooms_bestwestern_hotel_sfo',\n",
    "            'screen_ipod_nano_8gb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O function\n",
    "def import_dataset(file_name):\n",
    "    \"\"\"\n",
    "    input: file_name in file_list\n",
    "    \"\"\"\n",
    "    \n",
    "    dataFile = './dataset/' + str(file_name) + '.txt.data'\n",
    "    with open(dataFile, 'rb') as f:\n",
    "        contents = []\n",
    "        for line in f.readlines():\n",
    "            contents.append(line.strip().decode('utf-8'))\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing dataset\n",
      "document number:8\n"
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "print('Importing dataset')\n",
    "\n",
    "dataset = [] # 8-entry list, each corresponding to a file\n",
    "for file_name in file_list:\n",
    "    contents = import_dataset(file_name)\n",
    "    dataset.append(contents)\n",
    "    \n",
    "print('document number:' + str(len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A class Shingling that constructs k–shingles of a given length k (e.g., 10) from a given document, computes a hash value for each unique shingle, and represents the document in the form of an ordered set of its hashed k-shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function constructs k-shingles from a given text\n",
    "def shingling(text, k=5):\n",
    "    \"\"\"\n",
    "    input: text in dataset\n",
    "            k, shingle size.\n",
    "    \"\"\"\n",
    "    \n",
    "    # split the string into separate words\n",
    "    split_text = []\n",
    "    for review in text:\n",
    "        exclude = set(string.punctuation)\n",
    "        review = ''.join(ch for ch in review if ch not in exclude) # remove punctuation\n",
    "        review = review.lower() # convert all characters into lower characters\n",
    "        #for word in review.split():\n",
    "        #    split_text.append(word)\n",
    "        for character in list(review):\n",
    "            split_text.append(character)\n",
    "    \n",
    "    # k-shingle\n",
    "    shingle_list = []\n",
    "    for i in range(len(split_text)-k+1):\n",
    "        shingle_list.append(split_text[i:i+k])\n",
    "\n",
    "    # remove duplicates\n",
    "    shingle_list.sort()\n",
    "    shingle_no_dup = list(shingle_list for shingle_list,_ in itertools.groupby(shingle_list))\n",
    "    \n",
    "    # each sublist in shingle_no_dup represents a shingle\n",
    "    # convert the sublist into a string\n",
    "    shingle_strings = []\n",
    "    for index, shingle in enumerate(shingle_no_dup):\n",
    "        sum_string = shingle[0]\n",
    "        for i in range(1, len(shingle)):\n",
    "            sum_string = sum_string + ' ' + shingle[i]\n",
    "        shingle_strings.append(sum_string)\n",
    "        \n",
    "    return shingle_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shingling\n",
      "number of shingles in document 1 is 4838\n",
      "number of shingles in document 2 is 3228\n",
      "number of shingles in document 3 is 8507\n",
      "number of shingles in document 4 is 9910\n",
      "number of shingles in document 5 is 5360\n",
      "number of shingles in document 6 is 17206\n",
      "number of shingles in document 7 is 9323\n",
      "number of shingles in document 8 is 3247\n"
     ]
    }
   ],
   "source": [
    "# shingling all 8 texts\n",
    "print('Shingling')\n",
    "\n",
    "shingle_texts = []\n",
    "for text in dataset:\n",
    "    shingle_texts.append(shingling(text))\n",
    "    \n",
    "for i in range(len(shingle_texts)):\n",
    "    num = len(shingle_texts[i])\n",
    "    print(f'number of shingles in document {i+1} is {num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of shingles: 61619\n",
      "number of unique shingles: 30623\n"
     ]
    }
   ],
   "source": [
    "# flatting the list of all shingles\n",
    "flat_shingle_texts = np.hstack(np.array(shingle_texts))\n",
    "print('number of shingles:',flat_shingle_texts.shape[0])\n",
    "\n",
    "# get unique shingles\n",
    "unique_flat_shingle_texts = np.unique(flat_shingle_texts)\n",
    "max_value = unique_flat_shingle_texts.shape[0]\n",
    "print('number of unique shingles:', max_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hash unique shingles\n",
    "# build dictionary with {unique shingle: hash value}\n",
    "shingle_dict = {}\n",
    "for i in range(len(unique_flat_shingle_texts)):\n",
    "    shingle_dict[unique_flat_shingle_texts[i]] = hash(unique_flat_shingle_texts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# represents the document in the form of an ordered set of its hashed k-shingles\n",
    "shingle_int = shingle_texts\n",
    "for i in range(len(shingle_int)):\n",
    "    for j in range(len(shingle_int[i])):\n",
    "        shingle_int[i][j] = shingle_dict[shingle_texts[i][j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. A class CompareSets that computes the Jaccard similarity of two sets of integers – two sets of hashed shingles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute Jaccard similarity of two sets of hashed shingles\n",
    "def jaccard_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    input: text1, text2 are entries in shingle_int. eg: text1 = shingle_int[0]\n",
    "    \"\"\"\n",
    "    set1 = set(text1)\n",
    "    set2 = set(text2)\n",
    "    jaccard = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "    return jaccard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. A class MinHashing that builds a minHash signature (in the form of a vector or a set) of a given length n from a given set of integers (a set of hashed shingles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. A class CompareSignatures that estimates similarity of two integer vectors – minhash signatures – as a fraction of components, in which they agree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. (Optional task for extra 2 bonus) A class LSH that implements the LSH technique: given a collection of minhash signatures (integer vectors) and a similarity threshold t, the LSH class (using banding and hashing) finds all candidate pairs of signatures that agree on at least fraction t of their components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
